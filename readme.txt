ОПИСАНИЕ
- используется python 2.7
- parser/main.py запускается с командной строки, принимает первым параметром URL страницы, которую нужно отпарсить (например:$ python main.py http://newsland.com/news/detail/id/1290064/). Результат - отпаршенная страница сохраненная в файл .txt в директории соответствующая структуре URL.

- для тестирования можно запустить скрипт parser.py без параметров, берет URL из файла testLink.txt

- requirement.txt - список дополнителных библиотек

- saver.py - класс для работы с файловой системой

- settings.py - настройки парсера для сайтов. Более подробная информация по настройке в этом файле.

- testLink.txt - файл со списком URL, на которых проводилось тестирование

- my_test_result - директория с результатами тестов

АЛГОРИТМ

Скрипт на входе получает URL и настройки из settings.py.
Получаем страницу по полученному URL
Отдельно отбираем с страницы заголовок и текст (библиотека BeautifulSoup) согласно настройкам settings.py
После этого скрипт обрабатывает заголовок и текст согласно настройкам из settings.py:
- очищается от мусорных блоков, коментариев
- конвертируются теги - <a> в [URL], <p> в абзацы
- далее текст очищается от всех остальных тегов
Полученный текст сохраняется в директорию согласно структуре URL, последний элемент URL становится именем файла + .txt

ДАЛЬНЕЙШЕЕ РАЗВИТИЕ ПРОГРАММЫ

В первую очередь следует обратить внимание на более тщательное форматирование текста.
Одним из самых важных направлений является уменьшение объема конфигурационного файла, т.е. сделать скрипт максимально универсальным, участие пользователя в настройке свести к минимуму.
Так же следует тщательно проработать возможные исключения, ошибки в данных и т.п. - сделать программу максимально отказоустойчивой.
В зависимости от целей, возможно прикрутить GUI.

